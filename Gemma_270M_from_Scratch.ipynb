{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPvjz+P/2X11NjU/OPxuaec",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PARTHIBAN-007/Build-LLM-From-Scratch/blob/main/Gemma_270M_from_Scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X1ia8BizcDC2"
      },
      "outputs": [],
      "source": [
        "!pip install datasets\n",
        "!pip install -U datasets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "ds = load_dataset(\"roneneldan/TinyStories\")"
      ],
      "metadata": {
        "id": "ZukdYXooea46"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenize the Dataset"
      ],
      "metadata": {
        "id": "YCRjOMzBel8u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tiktoken\n",
        "import tiktoken\n",
        "import os\n",
        "import numpy as np\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "encoder = tiktoken.get_encoding(\"gpt2\")\n",
        "\n",
        "def process(example):\n",
        "  ids = encoder.encode_ordinary(example[\"text\"])\n",
        "  output = {'ids': ids, 'len': len(ids)}\n",
        "  return output\n",
        "\n",
        "\n",
        "if not os.path.exists(\"train.bin\"):\n",
        "  tokenized = ds.map(\n",
        "      process,\n",
        "      remove_columns = [\"text\"],\n",
        "      desc = \"tokenizing the splits\",\n",
        "      num_proc = 8,\n",
        "  )\n",
        "\n",
        "  for split,dset in tokenized.items():\n",
        "    arr_len = np.sum(dset['len'], dtype = np.uint64)\n",
        "    filename = f'{split}.bin'\n",
        "    dtype = np.uint16\n",
        "    arr = np.memmap(filename, dtype = dtype, mode = \"w+\", shape = (arr_len,))\n",
        "    total_batches = 1024\n",
        "\n",
        "    idx = 0\n",
        "    for batch_idx in tqdm(range(total_batches), desc = f'writing {filename}'):\n",
        "      batch = dset.shard(num_shards = total_batches, index = batch_idx, contigious = True).with_format('numpy')\n",
        "      arr_batch = np.concatenate(batch['ids'])\n",
        "      arr[idx:idx+len(arr_batch)] = arr_batch\n",
        "      idx += len(arr_batch)\n",
        "    arr.flush()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1MekeikOeo5L",
        "outputId": "30fa6529-edd5-454f-d4fd-0108991ade34"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.12/dist-packages (0.12.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.12/dist-packages (from tiktoken) (2.32.4)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken) (2025.10.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_batch(split):\n",
        "  if split==\"train\":\n",
        "    data = np.memmap('train.bin',dtype=np.uint16,mode='r')\n",
        "  else:\n",
        "    data = np.memmap('validation.bin',dtype=np.uint16,mode = 'r')\n",
        "  ix = torch.randint(len(data)-block_size, (batch_size),)\n",
        "  x = torch.stack([torch.from_numpy((data[i:i+block_size]).astype(np.int64))for i in ix])\n",
        "  y = torch.stack([torch.from_numpy((data[i+1:i+1+block_size]).astype(np.int64)) for i in ix])\n",
        "  if device_type ==\"cuda\":\n",
        "    x,y = x.pin_memory().to(device,non_blocking = True) , y.pin_memory().to(device,non_blocking = True)\n",
        "  else:\n",
        "    x,y = x.to(device), y.to(device)\n",
        "  return x,y"
      ],
      "metadata": {
        "id": "kr6zYIT8eq1N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "def compute_rope_params(head_dim, theta_base = 10_000, context_length = 4096,dtype = torch.float32):\n",
        "  assert head_dim%2==0, \"Embedding dimension must be even\"\n",
        "\n",
        "  inv_freq = 1.0 / (theta_base ** (torch.arange(0,head_dim,2,dtype = dtype)[: (head_dim//2)].float()/head_dim))\n",
        "\n",
        "  positions = torch.arange(context_length,dtype = dtype)\n",
        "  angles = positions[:,None]  * inv_freq[None, :]\n",
        "\n",
        "  angles = torch.cat([angles,angles],dim=1)\n",
        "\n",
        "  cos = torch.cos(angles)\n",
        "  sin = torch.sin(angles)\n",
        "  return cos,sin\n",
        "\n",
        "def apply_rope(x,cos,sin):\n",
        "  batch_size, num_heads, seq_length ,  head_dim = x.shape\n",
        "  assert head_dim%2==0 , \"Head Dimension must be even\"\n",
        "\n",
        "  x1 = x[..., : head_dim//2]\n",
        "  x2 = x[..., : head_dim//2:]\n",
        "\n",
        "  cos = cos[:seq_len,:].unsqueeze(0).unsqueeze(0)\n",
        "  sin = sin[:seq_len,:].unsqueeze(0).unsqueeze(0)\n",
        "\n",
        "  rotated = torch.cat((-x2,x1),dim=-1)\n",
        "  x_rotated = (x*cos) + (rotated*sin)\n",
        "\n",
        "  return x_rotated.to(dtype = x.dtype)\n"
      ],
      "metadata": {
        "id": "pG2bAYb6Wb9f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "from dataclasses import dataclass\n",
        "import numpy as np\n",
        "from tqdm.auto import tqdm\n",
        "from contextlib import nullcontext\n",
        "import os\n",
        "\n",
        "class RMSNorm(nn.Module):\n",
        "  def __init__(self,emb_dim,eps=1e-6,bias = False):\n",
        "    super.__init__()\n",
        "    self.eps = eps\n",
        "    self.scale = nn.Parameter(torch.zeros(emb_dim))\n",
        "    self.shift = nn.Parameters(torch.zeros(emb_dim)) if bias else None\n",
        "\n",
        "  def forward(self,x):\n",
        "    input_dtype = x.dtype\n",
        "    x_f = x.float()\n",
        "    var = x_f.pow(2).mean(dim=-1,keepdim = True)\n",
        "    x_norm = x_f * torch.rsqrt(var + self.eps)\n",
        "    out = x_norm * (1.0 + self.scale.float())\n",
        "\n",
        "    if self.shift is not None:\n",
        "      out = out + self.shift.float()\n",
        "\n",
        "    return out.to(input_type)\n",
        "\n",
        "class GroupedQueryAttention(nn.Module):\n",
        "  def __init__(\n",
        "      self,d_in, num_heads, num_kv_groups, head_dim = None,qk_norm = False,\n",
        "      query_pre_attn_scalar = None, dtype =None,\n",
        "  ):\n",
        "    super.__init__()\n",
        "    assert num_heads % num_kv_groups == 0 , \"num_heads must be divisible by num_kv_groups\"\n",
        "\n",
        "    self.num_heads = num_heads\n",
        "    self.num_kv_groups = num_kv_groups\n",
        "    self.group_size = num_heads // num_kv_groups\n",
        "\n",
        "    if head_dim is None:\n",
        "      assert d_in % num_heads == 0 , \"d_in must be divisible by num_heads\"\n",
        "      head_dim = d_in //num_heads\n",
        "\n",
        "    self.head_dim = head_dim\n",
        "    self.d_out = num_heads * head_dim\n",
        "\n",
        "    self.w_query = nn.Linear(d_in,self.d_out , bias = False,dtype= dtype)\n",
        "    self.w_key = nn.Linear(d_in,num_kv_groups * head_dim , bias = False,dtype= dtype)\n",
        "    self.w_value = nn.Linear(d_in,num_kv_groups * head_dim , bias = False,dtype= dtype)\n",
        "\n",
        "    self.out_proj = nn.Linear(self.d_out , d_in , bias = False, dtype= dtype)\n",
        "\n",
        "    if qk_norm:\n",
        "      self.q_norm = RMSNorm(head_dim , eps = 1e-6)\n",
        "      self.k_norm = RMSNorm(head_dim , eps = 1e-6)\n",
        "    else:\n",
        "      self.q_norm = self.k_norm = None\n",
        "\n",
        "    if query_pre_attn_scaler is not None:\n",
        "      self.scaling = (query_pre_attn_scaler) ** -0.5\n",
        "    else:\n",
        "      self.scaling = (head_dim) ** -0.5\n",
        "\n",
        "  def forward(self,x, mask,cos,sin):\n",
        "    b , num_tokens, _  = x.shape\n",
        "\n",
        "    queries = self.w_query(x)\n",
        "    keys = self.w_key(x)\n",
        "    values = self.w_value(x)\n",
        "\n",
        "\n",
        "    queries = queries.view(b, num_tokens , self.num_heads,self.head_dim).transpose(1,2)\n",
        "    keys = keys.view(b,num_tokens, self.num_kv_groups,self.head_dim).transpose(1,2)\n",
        "    values = values.view(b,num_tokens , self.num_kv_groups,self.head_dim).transpose(1,2)\n",
        "\n",
        "    if self.q_norm:\n",
        "      queries = self.q_norm(queries)\n",
        "    if self.k_norm:\n",
        "      keys = self.k_norm(keys)\n",
        "\n",
        "    queries = apply_rope(queries,cos,sin)\n",
        "    keys = apply_rope(keys,cos,sin)\n",
        "\n",
        "    keys = keys.repeat_interleave(self.group_size,dim = 1)\n",
        "    values = values.repeat_interleave(self.group_size, dim =1)\n",
        "\n",
        "    queries = queries * self.scaling\n",
        "\n",
        "    attn_scores = queries @ keys.transpose(2,3)\n",
        "    attn_scores = attn_scores.masked_fill(mask,-torch.inf)\n",
        "    attn_weights = torch.softman(attn_scores,dim=-1)\n",
        "\n",
        "    context = (attn_weights @ values).transpose(1,2).reshape(b,num_tokens,self.d_out)\n",
        "    return self.out_proj(context)\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "  def __init__(self,cfg):\n",
        "    super.__init__()\n",
        "    self.fc1 = nn.Linear(cfg[\"emb_dim\"],cfg[\"hidden_dim\"],dtype = cfg[\"dtype\"],bias = False)\n",
        "    self.fc2 = nn.Linear(cfg[\"emb_dim\"],cfg[\"hidden_dim\"],dtype = cfg[\"dtype\"],bias = False)\n",
        "    self.fc3 = nn.Linear(cfg[\"hidden_dim\"],cfg[\"emb_dim\"],dtype = cfg[\"dtype\"],bias = False)\n",
        "\n",
        "  def forward(self,x):\n",
        "    x_fc1 = self.fc1(x)\n",
        "    x_fc2 = self.fc2(x)\n",
        "    x = nn.functional.gelu(x_fc1,approximate = \"tanh\") * x_fc2\n",
        "    return self.fc3(x)\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "  def __init__(self,cfg:dict,attn_type:str):\n",
        "    super.__init__()\n",
        "    self.attn_type = attn_type\n",
        "    self.attn = GroupedQuerryAttention(\n",
        "        d_in = cfg[\"emb_dim\"],\n",
        "        num_heads = cfg[\"n_heads\"],\n",
        "        num_kv_groups = cfg[\"n_kv_groups\"],\n",
        "        head_dim = cfg[\"head_dim\"],\n",
        "        qk_norm = cfg[\"qk_norm\"],\n",
        "        query_pre_attn_scalar = cfg[\"query_pre_attn_scalar\"],\n",
        "        dtype = cfg[\"dtype\"],\n",
        "    )\n",
        "\n",
        "    self.ff = FeedForward(cfg)\n",
        "    self.input_layernorm = RMSNorm(cfg[\"emb_dim\"],eps = 1e-6)\n",
        "    self.post_attention_layernorm = RMSNorm(cfg[\"emb_dim\"],eps = 1e-6)\n",
        "    self.pre_feedforward_layernorm = RMSNorm(cfg[\"emb_dim\"],eps = 1e-6)\n",
        "    self.post_feedforward_layernorm = RMSNorm(cfg[\"emb_dim\"] , eps = 1e-6)\n",
        "\n",
        "  def forward(self,x,mask_global,mask_local,cos_global,sin_global,cos_local,sin_local):\n",
        "    shortcut = x\n",
        "    x = self.input_layernorm(x)\n",
        "\n",
        "    if self.attn_type ==\"sliding_attention\":\n",
        "      attn_mask = mask_local\n",
        "      cos = cos_local\n",
        "      sin = sin_local\n",
        "    else:\n",
        "      attn_masks = mask_global\n",
        "      cos = cos_global\n",
        "      sin = sin_global\n",
        "\n",
        "    x_attn = self.att(x,attn_mask,cos,sin)\n",
        "    x_attn = self.post_attention_layernorm(x_attn)\n",
        "    x = shortcut + x_attn\n",
        "\n",
        "    shortcut = x\n",
        "    x_ffn = self.pre_feedforward_layernorm(x)\n",
        "    x_ffn = self.ff(x_ffn)\n",
        "    x_ffn = self.post_feedforward_layernorm(x_ffn)\n",
        "    x = shortcut + x_ffn\n",
        "    return x\n",
        "\n",
        "class Gemma3Model(nn.Module):\n",
        "  def __init__(self,cfg):\n",
        "    super.__init__()\n",
        "    assert cfg[\"layer_types\"] is not None and len(cfg[\"layer_types\"]) == cfg[\"n_layers\"]\n",
        "\n",
        "    self.tok_emb = nn.Embedding(cfg[\"vocab_size\"],cfg[\"emb_dim\"],dtype = cfg[\"dtype\"])\n",
        "\n",
        "    self.blocks = nn.ModuleList([\n",
        "        TransformerBlock(cfg,attn_type) for attn_type in cfg[\"layer_types\"]\n",
        "    ])\n",
        "\n",
        "\n",
        "    self.final_norm = RMSNorm(cfg[\"emb_dim\"],eps = 1e-6)\n",
        "    self.out_head = nn.Linear(cfg[\"emb_dim\"], cfg[\"vocab_size\"],bias = False, dtype = cfg[\"dtype\"])\n",
        "    self.cfg = cfg\n",
        "\n",
        "    cos_local , sin_local = compute_rope_params(\n",
        "        head_dim = cfg[\"head_dim\"],\n",
        "        theta_base = cfg[\"rope_local_base\"],\n",
        "        context_length = cfg[\"context_length\"],\n",
        "        dtype = torch.float32,\n",
        "    )\n",
        "\n",
        "    cos_global , sin_global = compute_rope_params(\n",
        "        head_dim = cfg[\"head_dim\"],\n",
        "        theta_base = cfg[\"rope_global_base\"],\n",
        "        context_length = cfg[\"context_length\"],\n",
        "        dtype = torch.float32,\n",
        "    )\n",
        "    self.register_buffer(\"cos_local\", cos_local , persistent = False)\n",
        "    self.register_buffer(\"sin_local\", sin_local , persistent = False)\n",
        "    self.register_buffer(\"cos_global\", cos_global , persistent = False)\n",
        "    self.register_buffer(\"sin_global\", sin_global , persistent = False)\n",
        "\n",
        "  def _create_masks(self,seq_len, device):\n",
        "    ones = torch.ones((seq_len,seq_len),dtype = torch.bool, device =device)\n",
        "\n",
        "    mask_global = torch.triu(ones,diagonal = 1)\n",
        "    far_past = torch.triu(ones,diagonal = self.cfg[\"sliding_window\"])\n",
        "    mask_local  = mask_local | far_past\n",
        "    return mask_global , mask_local\n",
        "\n",
        "  def forward(self,input_ids,targets = None):\n",
        "    b,seq_len  = input_ids.shape\n",
        "    x = self.tok_emb(input_ids) * (self.cfg[\"emb_dim\"]**0.5)\n",
        "    mask_global , mask_local = self._create_masks(seq_len,x.device)\n",
        "\n",
        "    for block in self.blocks:\n",
        "      x = block(\n",
        "          x,\n",
        "          mask_global = mask_global,\n",
        "          mask_local= mask_local,\n",
        "          cos_global = self.cos_global,\n",
        "          sin_global = self.sin_global,\n",
        "          cos_local = self.cos_local,\n",
        "          sin_local = self.sin_local,\n",
        "      )\n",
        "    x = self.final_norm(x)\n",
        "    logits = self.out_head(x.to(self.cfg[\"dtype\"]))\n",
        "    loss = None\n",
        "    if targets is not None:\n",
        "      loss = F.cross_entropy(logits.reshape(-1,logits.size(-1)),targets.reshape(-1))\n",
        "    return logits,loss\n",
        "\n",
        "\n",
        "  @torch.no_grad()\n",
        "  def generate(self,idx,max_new_tokens,temperature = 1.0,top_k = None):\n",
        "    for _ in range(max_new_tokens):\n",
        "      ctx_len = self.cfg[\"context_length\"]\n",
        "      idx_cond = idx if idx.size(1) <= ctx_len else idx[:,-ctx_len:]\n",
        "      logits , _ = self(idx_cond)\n",
        "      logits = logits[:,-1,:]/ temperature\n",
        "      if top_k is not None:\n",
        "        v , _ = torch.topk(logits, min(top_k,logits.size(-1)))\n",
        "        logits[logits< v[:,[-1]]] = float('-inf')\n",
        "      probs = F.softmax(logits,dim=-1)\n",
        "      idx_next = torch.multinomial(probs,num_samples = 1)\n",
        "      idx  = torch.cat((idx,idx_next),dim=1)\n",
        "    return idx\n",
        "\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "gIHjtcVcYRre"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "GEMMA3_CONFIG_270M= {\n",
        "    \"vocab_size\": 50257,\n",
        "    \"context_length\": 32768,\n",
        "    \"emb_dim\": 640,\n",
        "    \"n_heads\": 4,\n",
        "    \"n_layers\": 18,\n",
        "    \"hidden_dim\": 2048,\n",
        "    \"head_dim\": 256,\n",
        "    \"qk_norm\": True,\n",
        "    \"n_kv_groups\": 1,\n",
        "    \"rope_local_base\": 10000.0,\n",
        "    \"rope_base\": 1000000.0,\n",
        "    \"sliding_window\": [\n",
        "        \"sliding_attention\",\n",
        "        \"sliding_attention\",\n",
        "        \"sliding_attention\",\n",
        "        \"sliding_attention\",\n",
        "        \"sliding_attention\",\n",
        "        \"sliding_attention\",\n",
        "        \"sliding_attention\",\n",
        "        \"full_attention\",\n",
        "        \"sliding_attention\",\n",
        "        \"sliding_attention\",\n",
        "        \"sliding_attention\",\n",
        "        \"sliding_attention\",\n",
        "        \"sliding_attention\",\n",
        "        \"sliding_attention\",\n",
        "        \"sliding_attention\",\n",
        "        \"full_attention\",\n",
        "        \"sliding_attention\",\n",
        "        \"sliding_attention\",\n",
        "        \"sliding_attention\",\n",
        "        \"sliding_attention\",\n",
        "        \"sliding_attention\",\n",
        "        \"sliding_attention\",\n",
        "        \"sliding_attention\",\n",
        "        \"full_attention\",\n",
        "    ],\n",
        "    \"dtype\": torch.bfloat16,\n",
        "    \"query_pre_attention_scalar\": 256,\n",
        "}\n",
        "\n",
        "torch.manual_seed(123)\n",
        "modek = Gemma3Model(GEMMA3_CONFIG_270M)"
      ],
      "metadata": {
        "id": "1Y-7oqUUszMW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loss Function"
      ],
      "metadata": {
        "id": "cL7CfPOd46_k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def estimate_loss(model):\n",
        "  out = {}\n",
        "  model.eval()\n",
        "  with torch.inference_mode():\n",
        "    for split in ['train','val']:\n",
        "      losses = torch.zeros(eval_iters)\n",
        "      for k in range(eval_iters):\n",
        "        x,y = get_batch(split)\n",
        "        with ctx:\n",
        "          logits, loss = model(x,y)\n",
        "        losses[k] = loss.item()\n",
        "      out[split] = losses.mean()\n",
        "  model.train()\n",
        "  return out"
      ],
      "metadata": {
        "id": "FaUZbtff48uz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}